{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Lorem ipsum dolor sit amet, consectetur adipis...\n",
      "1  Pellentesque habitant morbi tristique senectus...\n",
      "2  Quisque suscipit elit nec convallis ullamcorpe...\n",
      "3  Lorem ipsum dolor sit amet, çonsectetur adipis...\n",
      "4  Pellentesque habitant morbi tristique senectus...\n"
     ]
    }
   ],
   "source": [
    "from text_analyzer.file_manager import FileManager\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import json\n",
    "import re\n",
    "\n",
    "class Analyzer:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = self._preprocess_data(data)\n",
    "        self.char_number = self._calculate_char_number()\n",
    "        self.word_number = self._calculate_word_number()\n",
    "        self.non_alpha_chars = self._count_non_alpha_chars()\n",
    "        self.ngram_dict = self._calculate_most_used_ngrams()\n",
    "    \n",
    "\n",
    "    def _preprocess_data(self, data):\n",
    "        \"\"\"Preprocess data and add preprocessed text column to data\"\"\"\n",
    "        data['preprocessed_text'] = data['text'].apply(lambda x: x.lower())\n",
    "        data['processed_text'] = data['text'].apply(self._preprocessing)\n",
    "        return data\n",
    "    \n",
    "    def _preprocessing(self, text):\n",
    "        \"\"\"lower, remove punctuations, remove numbers, remove whitespaces. Use regex.\"\"\"\n",
    "        text = text.lower()\n",
    "\n",
    "        text = re.sub(r'[^\\w\\s]','', text)\n",
    "        text = re.sub(r'\\d','', text)\n",
    "        text = re.sub(r'\\s+',' ', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def _calculate_char_number(self):\n",
    "        \"\"\"calculate number of chars for each doc. Return min, max, mean values as dict\"\"\"\n",
    "        self.data['n_char'] = self.data['text'].apply(lambda x: len(x))\n",
    "        return self.data['n_char'].agg(['min', 'mean', 'max']).to_dict()\n",
    "    \n",
    "    def _calculate_word_number(self):\n",
    "        \"\"\"calculate number of words for each doc. Return min, max, mean values as dict\"\"\"\n",
    "        self.data['n_word'] = self.data['text'].apply(lambda x: len(x.split()))\n",
    "        return self.data['n_word'].agg(['min', 'mean', 'max']).to_dict()\n",
    "    \n",
    "    def _count_non_alpha_chars(self, n=10):\n",
    "        non_alpha_chars = Counter()\n",
    "        total_non_alpha_count = 0\n",
    "        \n",
    "        for text in self.data['text']:\n",
    "            non_alpha = [char for char in text if not char.isalpha() and not char.isspace()]\n",
    "            non_alpha_chars.update(non_alpha)\n",
    "            total_non_alpha_count += len(non_alpha)\n",
    "        \n",
    "        return {\"10_most_common_with_freq\": dict(non_alpha_chars.most_common(n)),\n",
    "                \"total_count\": total_non_alpha_count}\n",
    "    \n",
    "    def _calculate_most_used_ngrams(self, n_min=1, n_max=3, first_k=10):\n",
    "        \"\"\"calculate most used ngrams for each doc. Return dict of ngrams. Keys are n values. Values are dicts of ngrams and their frequencies. n_min and n_max are used to determine n values. first_k is used to determine how many ngrams will be returned.\"\"\"\n",
    "        text_splitted = [word for sentence in self.data['preprocessed_text'].tolist() for word in sentence.split()]\n",
    "        ngram_dict = dict()\n",
    "        for n in range(n_min, n_max+1):\n",
    "            ngram = Counter(ngrams(text_splitted, n)).most_common(first_k)\n",
    "            ngram_freq = {\" \".join(phrase): freq for phrase, freq in ngram}\n",
    "            ngram_dict[n] = ngram_freq\n",
    "\n",
    "        return ngram_dict\n",
    "\n",
    "    def generate_word_cloud(self, use_processed_data=True, save=False, output_name='word_cloud.png'):\n",
    "        \"\"\"import wordcloud library and generate a single word cloud with all the documents\"\"\"\n",
    "        from wordcloud import WordCloud\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        data = self.data['processed_text'].tolist() if use_processed_data else self.data['text'].tolist()\n",
    "\n",
    "        world_cloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(\" \".join(data))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(world_cloud, interpolation='bilinear')\n",
    "\n",
    "        if save:\n",
    "            world_cloud.to_file(output_name)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _get_simple_stats(self):\n",
    "\n",
    "        n_char_data = self.calculate_char_number()\n",
    "        n_word_data = self.calculate_word_number()\n",
    "        non_alpha_data = self.count_non_alpha_chars()\n",
    "\n",
    "        #TODO ways to get number for ï like letters.\n",
    "        #TODO: # of links\n",
    "        #TODO: # of phone numbers\n",
    "    \n",
    "    def print_stats(self, pretty=True):\n",
    "        analyzer_dict = self.__dict__.copy()\n",
    "        analyzer_dict.pop('data')\n",
    "        \n",
    "        res = json.dumps(analyzer_dict, indent=4 if pretty else None)\n",
    "        print(res)\n",
    "        \n",
    "    def to_json(self, output_name):\n",
    "        analyzer_dict = self.__dict__.copy()\n",
    "        analyzer_dict.pop('data')\n",
    "        \n",
    "        if not output_name.endswith('.json'):\n",
    "            output_name+='.json'\n",
    "        \n",
    "        with open(output_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analyzer_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# fm = FileManager()\n",
    "# data = fm.read_txt('/Users/fatih/Desktop/val.txt')\n",
    "# print(data.iloc[-1, :])\n",
    "\n",
    "# analyzer = Analyzer(data)\n",
    "# analyzer.print_stats()\n",
    "# analyzer.generate_word_cloud(save=True, output_name='word_cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlyz = Analyzer(data)\n",
    "# anlyz.data.apply(anlyz.calculate_simple_stats, axis=1)\n",
    "# anlyz.count_non_ascii_chars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m anlyz\u001b[39m.\u001b[39;49mgenerate_word_cloud()\n",
      "\u001b[1;32m/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_word_cloud\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"import wordcloud library and generate a single word cloud with all the documents\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fatih/Desktop/Projects/text_analyzer/development.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     world_cloud \u001b[39m=\u001b[39m WordCloud(width\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m, background_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m, min_font_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mgenerate(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mprocessed_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "anlyz.generate_word_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"char_number\": {\n",
      "        \"min\": 390.0,\n",
      "        \"mean\": 507.75,\n",
      "        \"max\": 610.0\n",
      "    },\n",
      "    \"word_number\": {\n",
      "        \"min\": 58.0,\n",
      "        \"mean\": 74.25,\n",
      "        \"max\": 91.0\n",
      "    },\n",
      "    \"non_alpha_chars\": {\n",
      "        \"10_most_common_with_freq\": {\n",
      "            \".\": 83,\n",
      "            \",\": 38,\n",
      "            \"5\": 4,\n",
      "            \"/\": 2,\n",
      "            \"1\": 2,\n",
      "            \":\": 1,\n",
      "            \"+\": 1,\n",
      "            \"(\": 1,\n",
      "            \")\": 1,\n",
      "            \"2\": 1\n",
      "        },\n",
      "        \"total_non_alpha_count\": 139\n",
      "    },\n",
      "    \"ngram_dict\": {\n",
      "        \"1\": {\n",
      "            \"sed\": 15,\n",
      "            \"nec\": 15,\n",
      "            \"in\": 15,\n",
      "            \"vitae\": 13,\n",
      "            \"ut\": 12,\n",
      "            \"ac\": 12,\n",
      "            \"at\": 11,\n",
      "            \"eu\": 11,\n",
      "            \"sapien\": 10,\n",
      "            \"arcu\": 10\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"nulla facilisi.\": 6,\n",
      "            \"ut vestibulum\": 4,\n",
      "            \"nec sapien\": 4,\n",
      "            \"lorem ipsum\": 3,\n",
      "            \"ipsum dolor\": 3,\n",
      "            \"dolor sit\": 3,\n",
      "            \"nec turpis\": 3,\n",
      "            \"ac fringilla\": 3,\n",
      "            \"vivamus consequat\": 3,\n",
      "            \"ac ultricies\": 3\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"lorem ipsum dolor\": 3,\n",
      "            \"ipsum dolor sit\": 3,\n",
      "            \"dolor sit amet,\": 2,\n",
      "            \"amet, consectetur adipiscing\": 2,\n",
      "            \"ligula. fusce tincidunt\": 2,\n",
      "            \"fusce tincidunt odio\": 2,\n",
      "            \"tincidunt odio vitae\": 2,\n",
      "            \"odio vitae nisl\": 2,\n",
      "            \"vitae nisl commodo\": 2,\n",
      "            \"nisl commodo hendrerit.\": 2\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "anlyz.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlyz.to_json('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('this', 'is'), 2), (('this', 'sentence'), 2), (('sentence', 'is'), 2), (('hello', 'this'), 1), (('is', 'a'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "testing = ['hello this is a testing sentence', 'this sentence is not related with anything', 'this sentence is created for test purposes', 'this is another list of sentences', 'another sentence bla bla']\n",
    "testing = \" \".join(testing)\n",
    "testing_splitted = [word for word in testing.split()]\n",
    "ngram = ngrams(testing_splitted, 2)\n",
    "ngram = Counter(ngram).most_common(5)\n",
    "print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is 2\n",
      "this sentence 2\n",
      "sentence is 2\n",
      "hello this 1\n",
      "is a 1\n"
     ]
    }
   ],
   "source": [
    "for phrase, freq in ngram:\n",
    "    print(\" \".join(phrase), freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('this', 'is'): 2,\n",
       " ('this', 'sentence'): 2,\n",
       " ('sentence', 'is'): 2,\n",
       " ('hello', 'this'): 1,\n",
       " ('is', 'a'): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'sed': 15,\n",
       "  'nec': 15,\n",
       "  'in': 15,\n",
       "  'vitae': 13,\n",
       "  'ut': 12,\n",
       "  'ac': 12,\n",
       "  'at': 11,\n",
       "  'eu': 11,\n",
       "  'sapien': 10,\n",
       "  'arcu': 10},\n",
       " 2: {'nulla facilisi.': 6,\n",
       "  'ut vestibulum': 4,\n",
       "  'nec sapien': 4,\n",
       "  'lorem ipsum': 3,\n",
       "  'ipsum dolor': 3,\n",
       "  'dolor sit': 3,\n",
       "  'nec turpis': 3,\n",
       "  'ac fringilla': 3,\n",
       "  'vivamus consequat': 3,\n",
       "  'ac ultricies': 3},\n",
       " 3: {'lorem ipsum dolor': 3,\n",
       "  'ipsum dolor sit': 3,\n",
       "  'dolor sit amet,': 2,\n",
       "  'amet, consectetur adipiscing': 2,\n",
       "  'ligula. fusce tincidunt': 2,\n",
       "  'fusce tincidunt odio': 2,\n",
       "  'tincidunt odio vitae': 2,\n",
       "  'odio vitae nisl': 2,\n",
       "  'vitae nisl commodo': 2,\n",
       "  'nisl commodo hendrerit.': 2}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anlyz.ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
